{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Reshape \n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Autoencoder\n",
    "\n",
    "In this exercise we will be using an autoencoder to first compress hand-written digit images from the MNIST dataset down to lower dimensional representations and then expand them back the original images.<br>\n",
    "![AE](fig/AE.jpg)\n",
    "<br>\n",
    "To keep things simple we will use dense layers, so no convolutions here.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load in the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape: (28, 28)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# normalize image data\n",
    "x_train = x_train/255.0\n",
    "x_test = x_test/255.0 \n",
    "\n",
    "# print image dimensions\n",
    "print(f'image shape: {x_train[0].shape}')\n",
    "\n",
    "# Plot example image from x_train\n",
    "plt.imshow(x_train[0], cmap = \"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the first part of the autoencoder: the encoder model<br>\n",
    "The enocder model compresses the input image down to a lower dimensional latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "=================================================================\n",
      "Total params: 110,816\n",
      "Trainable params: 110,816\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# pick a size for the latent dimension\n",
    "# how low can you go and still get good results?\n",
    "# keep in mind the orignal image is 28x28 = 784 pixels\n",
    "# 32 might be a good first value to try\n",
    "# your code here\n",
    "LATENT_SIZE = 32\n",
    "\n",
    "# Note how sequential models can also be passed a list of layers\n",
    "# This can be more concise than using add()\n",
    "encoder = Sequential([\n",
    "    Flatten(input_shape = (28, 28)), # we need to flatten the 2D image before the Dense layer\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    # specify the size of the latent dimension\n",
    "    # your code here\n",
    "    Dense(32, activation='relu'),\n",
    "])\n",
    "\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create the 2nd half of the autoencoder: the decoder<br>\n",
    "The decoder expands an image representation in the latent space back to the full dimensions of the original input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 784)               101136    \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 28, 28)            0         \n",
      "=================================================================\n",
      "Total params: 111,568\n",
      "Trainable params: 111,568\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder = Sequential([\n",
    "    Dense(64, input_shape = (LATENT_SIZE,), activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    # specify a reasonable output activation\n",
    "    # your code here\n",
    "    Dense(784, activation='relu'),\n",
    "    Reshape((28, 28)) # note the reshape to make the output 2D\n",
    "])\n",
    "\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we combine the encoder and decoder into the autoencoder.<br>\n",
    "The autoencoder shrinks the image down to the latent space representation and then expands it again to the original dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "sequential (Sequential)      (None, 32)                110816    \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 28, 28)            111568    \n",
      "=================================================================\n",
      "Total params: 222,384\n",
      "Trainable params: 222,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img = Input(shape = (28, 28)) # input\n",
    "latent_vector = encoder(img) # latent space\n",
    "output = decoder(latent_vector) # output\n",
    "\n",
    "# here we use the alternative Model constructor where we specify the model's input and output\n",
    "# your code here\n",
    "autoencoder = Model(inputs = img, outputs = output)\n",
    "\n",
    "# your code here\n",
    "# choose a sensible loss function for 'reconstruction error'\n",
    "autoencoder.compile(\"nadam\", loss = 'mse')\n",
    "\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even a simple neural network like this can take a minute or two to train.<br>\n",
    "For this reason you've been provided with some code that visualizes the model predictions on the test set after each epoch.<br>\n",
    "You need to specify\n",
    "1. the number of epochs you'd like to train for and\n",
    "2. the predictor and target data used for train and validation\n",
    "\n",
    "**Hint:** *Autoencoders are 'self-supervised' and we are trying to minimize the reconstruction loss.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a number of epochs to train for\n",
    "# your code here\n",
    "EPOCHS = 100\n",
    "\n",
    "# Note: epoch 0 is before any fitting\n",
    "for epoch in range(EPOCHS+1):\n",
    "    fig, axs = plt.subplots(4, 4)\n",
    "    rand = x_test[np.random.randint(0, 10000, 16)].reshape((4, 4, 1, 28, 28))\n",
    "    \n",
    "    for i in range(4):\n",
    "        for j in range(0,4,2):\n",
    "            axs[i, j].imshow(rand[i, j][0], cmap = \"gray\")\n",
    "            axs[i, j].axis(\"off\")\n",
    "            axs[i, j+1].imshow(autoencoder.predict(rand[i, j])[0], cmap = \"gray\")\n",
    "            axs[i, j+1].axis(\"off\")\n",
    "            if i == 0:\n",
    "                axs[i, j].set_title('test')\n",
    "                axs[i, j+1].set_title('pred')\n",
    "    \n",
    "    plt.subplots_adjust(wspace = 0, hspace = 0)\n",
    "    plt.show()\n",
    "    print(\"-----------\", \"EPOCH\", epoch, \"-----------\")\n",
    "    # specify predictors and targets for train and validation\n",
    "    # your code here\n",
    "    autoencoder.fit(x=x_train,\n",
    "                    y=y_train,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did you do? You can experiment with:\n",
    "* different latent size\n",
    "* more/larger layers\n",
    "* regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
